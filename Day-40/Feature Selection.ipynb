{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977e51e2-0201-4256-938d-73bd564fc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9655f90d-ae4e-44be-a6f4-1305d12ce057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom dataset for testing\n",
    "X, y = make_classification(\n",
    "    n_samples=800, # total rows\n",
    "    n_features=10, # total columns\n",
    "    n_informative=5, # informative features\n",
    "    n_redundant=0,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, stratify=y, random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f93f38-29e0-4136-81ef-29f3bfd604a8",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f296f-2cc7-4d7f-96a1-ecd6257f4547",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting a subset of relevant and informative features from a larger set of available features for use in machine learning algorithms. The aim is to reduce the dimensionality of the data and improve the accuracy and efficiency of the model.\n",
    "\n",
    "There are several techniques of feature selection. Let's take a look into a two most popular techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95549617-c4f9-46b7-83c0-2d6de5f95429",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "\n",
    "Forward feature selection involves **starting with an empty set of features and iteratively adding one feature at a time** based on their individual performance in predicting the outcome variable. This process continues until a stopping criterion is met, such as reaching a predefined number of features or a specific level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69eb82d-318c-4729-a19b-296682f8a858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6adf48-bb18-4c9a-8294-39be8f7c44ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature (forward):  [9]  Score:  0.66875\n",
      "Selected Feature (forward):  [9, 0]  Score:  0.6125\n",
      "Selected Feature (forward):  [9, 0, 5]  Score:  0.7375\n",
      "Selected Feature (forward):  [9, 0, 5, 8]  Score:  0.7875\n",
      "Selected Feature (forward):  [9, 0, 5, 8, 1]  Score:  0.775\n",
      "Selected Feature (forward):  [9, 0, 5, 8, 1, 2]  Score:  0.775\n",
      "Selected Feature (forward):  [9, 0, 5, 8, 1, 2, 3]  Score:  0.76875\n",
      "Selected Feature (forward):  [9, 0, 5, 8, 1, 2, 3, 4]  Score:  0.775\n",
      "Selected Feature (forward):  [9, 0, 5, 8, 1, 2, 3, 4, 6]  Score:  0.775\n",
      "Selected Feature (forward):  [9, 0, 5, 8, 1, 2, 3, 4, 6, 7]  Score:  0.78125\n"
     ]
    }
   ],
   "source": [
    "selected_feature = []\n",
    "\n",
    "for each in range(X_train.shape[1]):\n",
    "    best_acc = 0\n",
    "    best_feature = None\n",
    "    \n",
    "    for j in range(X_train.shape[1]):\n",
    "        \n",
    "        if j not in selected_feature:\n",
    "            \n",
    "            features = selected_feature + [j]\n",
    "            \n",
    "            model = LogisticRegression()\n",
    "            model.fit(X_train[:, features] , y_train)\n",
    "            accuracy = model.score(X_test[:, features], y_test)\n",
    "\n",
    "            if accuracy > best_acc:\n",
    "                best_acc = accuracy\n",
    "                best_feature = j\n",
    "                \n",
    "    selected_feature.append(best_feature)\n",
    "\n",
    "    print(\"Selected Feature (forward): \", selected_feature, \" Score: \",  accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280ce1e-f3c5-4bc2-9dfd-6dcf83453d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a158afe0-c7a2-4290-874f-03e79307a724",
   "metadata": {},
   "source": [
    "### Backward Feature selection\n",
    "\n",
    "Backward feature selection, on the other hand, **starts with all available features and iteratively removes one feature at a time based on their individual performance** in predicting the outcome variable. This process continues until a stopping criterion is met, such as reaching a predefined number of features or a specific level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d69ba3-6815-4c37-981f-11bceb7a2d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature (Backward):  [0, 1, 2, 3, 4, 5, 6, 7, 8]  Score:  0.675\n",
      "Selected Feature (Backward):  [0, 1, 2, 3, 4, 6, 7, 8]  Score:  0.65625\n",
      "Selected Feature (Backward):  [0, 1, 2, 3, 4, 6, 7]  Score:  0.51875\n",
      "Selected Feature (Backward):  [0, 1, 3, 4, 6, 7]  Score:  0.51875\n",
      "Selected Feature (Backward):  [0, 1, 4, 6, 7]  Score:  0.51875\n",
      "Selected Feature (Backward):  [1, 4, 6, 7]  Score:  0.53125\n",
      "Selected Feature (Backward):  [1, 4, 7]  Score:  0.475\n",
      "Selected Feature (Backward):  [1, 4]  Score:  0.475\n",
      "Selected Feature (Backward):  [1]  Score:  0.43125\n"
     ]
    }
   ],
   "source": [
    "selected_feature = list(range(X_train.shape[1]))\n",
    "\n",
    "for each in range(X_train.shape[1] - 1):\n",
    "    worst_acc = 1\n",
    "    worst_feature = None\n",
    "    \n",
    "    for j in selected_feature:\n",
    "                    \n",
    "        features = selected_feature.copy()\n",
    "        features.remove(j)\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train[:, features] , y_train)\n",
    "        accuracy = model.score(X_test[:, features], y_test)\n",
    "\n",
    "        if accuracy < worst_acc:\n",
    "            worst_acc = accuracy\n",
    "            worst_feature = j\n",
    "                \n",
    "    selected_feature.remove(worst_feature)\n",
    "\n",
    "    print(\"Selected Feature (Backward): \", selected_feature, \" Score: \",  accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a15726-f014-4523-896b-7f7e20ef87b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbb59ee-b69a-4372-945c-48bfac56a6a1",
   "metadata": {},
   "source": [
    "Both forward and backward feature selection have their own advantages and limitations. Forward feature selection tends to be more computationally efficient and is more likely to identify relevant features that may be missed in backward selection. However, it may also include irrelevant features that may not contribute to the overall accuracy of the model.\n",
    "\n",
    "In contrast, backward feature selection tends to produce more parsimonious models that may be easier to interpret and have better generalizability. However, it may also remove important features that may have a significant impact on the model's accuracy.\n",
    "\n",
    "Ultimately, the choice between forward and backward feature selection depends on the specific needs and characteristics of the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ffad7-561e-4995-b6c5-ad7d2bfe9c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
